{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7aefa78-f8da-4984-8cf1-c032400a5815",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "from urllib.request import urlopen\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.functions import udf,  mean, stddev, broadcast, col, split, lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassificationModel\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "import sklearn \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"MLlib lab\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c41291-083c-4559-9143-e67a78c4983d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2848b62-7b35-45d0-ba86-064df0df05c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: DataFrame[address: string, year: int, day: int, length: int, weight: double, count: int, looped: int, neighbors: int, income: double, label: string]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.csv(\"/mnt/team13/BitcoinHeistData.csv\", inferSchema=True, header=True)\n",
    "\n",
    "# Cache your DataFrame\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f471d0-0200-4f37-87d4-cc25478b85ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### MERGE MINORITY LABLES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fff27c4-ccf1-463b-b236-f9d20558cbf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Count the number of samples for each label\n",
    "label_counts = df.groupBy(\"label\").count().withColumnRenamed('count', 'count(label)' )\n",
    "\n",
    "# Create a new column \"merged_label\" with the merged labels\n",
    "merged_labels = label_counts.withColumn(\n",
    "    \"merged_label\",\n",
    "    when(col(\"count(label)\") < 20, \"Others\").otherwise(col(\"label\"))\n",
    ")\n",
    "\n",
    "# Join the merged labels with the original DataFrame\n",
    "merged_df = df.join(merged_labels, \"label\")\n",
    "\n",
    "# Use the \"merged_label\" column as the new label\n",
    "new_df = merged_df.drop('label')\n",
    "new_df = new_df.drop('count(label)')\n",
    "new_df = new_df.withColumnRenamed('merged_label', 'label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb3409e1-3fa8-45a9-9b7e-1e0e94027210",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### ENCODE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13665b62-abe3-43a9-b917-c4a279845c6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encode address column ( string indexer doesnt work due to too many distinct values)\n",
    "from pyspark.sql.types import IntegerType\n",
    "# Get distinct addresses from new_df\n",
    "distinct_addresses = new_df.select('address').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Define a dictionary to map addresses to labels\n",
    "label_dict = dict(zip(distinct_addresses, range(len(distinct_addresses))))\n",
    "\n",
    "# Define a UDF to apply the label mapping\n",
    "label_udf = udf(lambda x: label_dict[x], IntegerType())\n",
    "\n",
    "# Apply the label mapping to the address column in new_df\n",
    "encoded_address = new_df.withColumn('addressIndex', label_udf('address'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1a4548-dab4-468a-b280-2382f24f44c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol='label', outputCol='labelIndex') \n",
    "encoded_df = indexer.fit(encoded_address).transform(encoded_address)\n",
    "\n",
    "encoded_df = encoded_df.drop('label', 'address')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e1a331-df8e-45f1-94e6-c8fad40c833a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### REMOVE OUTLIERS and SPLIT TRAIN AND TEST SET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da5b09ca-fc5a-4b46-bb93-f95d5004ca94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# split training and test set (stratified sampling)\n",
    "fractions = encoded_df.select(\"labelIndex\").distinct().withColumn(\"fraction\", lit(0.8)).rdd.collectAsMap()                                                   \n",
    "train = encoded_df.stat.sampleBy(\"labelIndex\", fractions, seed = 42)\n",
    "test = encoded_df.subtract(train)\n",
    "\n",
    "def z_score_udf(col):\n",
    "    return (col - broadcast_mean.value) / broadcast_stddev.value\n",
    "\n",
    "for column in train.columns[1:-2] : \n",
    "    threshold = 3\n",
    "    mean_val = train.agg(mean(column)).collect()[0][0]\n",
    "    stddev_val = train.agg(stddev(column)).collect()[0][0]\n",
    "    broadcast_mean = sc.broadcast(mean_val)\n",
    "    broadcast_stddev = sc.broadcast(stddev_val)\n",
    "    print(column)\n",
    "    print(broadcast_mean.value)\n",
    "    z_score_udf_spark = udf(z_score_udf, DoubleType())\n",
    "    train = train.withColumn(\"z_score\", z_score_udf_spark(train[column]))\n",
    "    train = train.withColumn(\"outlier\", F.when(train.z_score<= threshold, False).otherwise(F.when(train.z_score > threshold, False).otherwise(True)))\n",
    "    train = train.filter(train.outlier == False)\n",
    "    train = train.drop(\"z_score\", \"outlier\")\n",
    "    \n",
    "train.cache()\n",
    "test.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b690967-f70c-41c5-95d2-a6fc3f13fc6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_majority = train.filter(train.labelIndex == '0.0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ae0aca-a2ce-40b1-9138-9aee0bc2d61d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from pyspark.sql.types import StructType, StructField, FloatType\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "count_majority = train.filter(train.labelIndex == '0.0').count()\n",
    "majority_class_label = 0\n",
    "sample_num = int(count_majority*0.5/4)\n",
    "\n",
    "sampling_under = {0.0  : sample_num}\n",
    "data = train.repartition(4)  \n",
    "feature_cols = train.columns[:-1]\n",
    "label_col = train.columns[-1]\n",
    "def create_local_model():\n",
    "    return DecisionTreeClassifier()\n",
    "\n",
    "# Define the train_partition function to train the model on partitions\n",
    "def train_partition(partition_df):\n",
    "  # separate features and labels \n",
    "  X = partition_df[feature_cols]\n",
    "  y = partition_df[label_col]\n",
    "  # undersample majority class by approximately( 50 % )\n",
    "  undersample = RandomUnderSampler(sampling_strategy = sampling_under )\n",
    "  X_under, y_under = undersample.fit_resample(X, y)\n",
    "  local_model = create_local_model()\n",
    "  local_model.fit(X_under, y_under)\n",
    "    \n",
    "  return iter([local_model]) \n",
    "\n",
    "# convert partition data to Pandas DataFrame\n",
    "def partition_to_pandas(partition):\n",
    "    partition_data = [row.asDict() for row in partition]\n",
    "    return pd.DataFrame(partition_data)\n",
    "    \n",
    "# Step 7: Use the mapPartitions function to train the model on each partition\n",
    "partition_models = data.rdd.mapPartitions(lambda partition: train_partition(partition_to_pandas(partition)))\n",
    "\n",
    "# Step 8: Aggregate the models \n",
    "trained_models = partition_models.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6563c792-dcd3-4ad0-961a-77f49f367294",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ensemble_predict(models, X):\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        predictions.append(model.predict(X))\n",
    "    # Transpose the list of predictions so that each row represents predictions for a single sample\n",
    "    predictions = list(map(list, zip(*predictions)))\n",
    "\n",
    "    # Use majority voting to combine the predictions\n",
    "    ensemble_predictions = []\n",
    "    for prediction in predictions:\n",
    "        majority_vote = max(set(prediction), key=prediction.count)\n",
    "        ensemble_predictions.append(majority_vote)\n",
    "\n",
    "    return ensemble_predictions\n",
    "\n",
    "\n",
    "# Prepare the test data for prediction\n",
    "test_data = test.toPandas()\n",
    "X_test = test_data[feature_cols]\n",
    "\n",
    "# Make predictions using the ensemble method\n",
    "ensemble_predictions = ensemble_predict(trained_models, X_test)\n",
    "\n",
    "#  Evaluate the accuracy of the ensemble predictions\n",
    "from sklearn.metrics import f1_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "y_test = test_data[label_col]\n",
    "f1_weighted = f1_score(y_test, ensemble_predictions, average = 'weighted')\n",
    "print(\"f1 score (weighted):\", f1_weighted)\n",
    "f1_micro = f1_score(y_test, ensemble_predictions, average = 'micro')\n",
    "print(\"f1 score (micro):\", f1_micro)\n",
    "f1_macro = f1_score(y_test, ensemble_predictions, average = 'macro')\n",
    "print(\"f1 score (macro):\", f1_macro)\n",
    "f1 = f1_score(y_test, ensemble_predictions, average = None)\n",
    "print(\"f1 score (none):\", f1)\n",
    "cm = confusion_matrix(y_test, ensemble_predictions, labels = trained_models[0].classes_)\n",
    "fig, ax = plt.subplots(1,figsize=(20, 20))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels  =trained_models[0].classes_)\n",
    "disp.plot(ax = ax)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "local-undersample ONLY",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
